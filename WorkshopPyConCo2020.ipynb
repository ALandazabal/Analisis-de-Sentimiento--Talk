{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying sentiment analysis to your social network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Extract Data\n",
    "\n",
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    "import xlsxwriter\n",
    "\n",
    "#import twitterCredentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = twitterCredentials.api_key\n",
    "api_secret = twitterCredentials.api_secret\n",
    "access_token = twitterCredentials.access_token\n",
    "access_secret = twitterCredentials.access_secret\n",
    "\n",
    "auth = OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    "api = tweepy.API(auth, wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = xlsxwriter.Workbook('Test1.xlsx')\n",
    "sheet = book.add_worksheet('Data')\n",
    "row = 0\n",
    "col = 0\n",
    " \n",
    "header = [\"Date\", \"Day\", \"Tweet\", \"Retweets\", \"Likes\"]\n",
    "for title in header:\n",
    "    sheet.write(row, col, title)\n",
    "    col += 1\n",
    "row = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"cnnespan\"\n",
    "\n",
    "timeline = api.user_timeline(id=name, count=500 )\n",
    "\n",
    "for tweet in timeline:\n",
    "    print(tweet.text)\n",
    "    data = [tweet.created_at.date().strftime(\"%b %d %Y \"), tweet.created_at.date().strftime(\"%b %d\"), tweet.text, tweet.retweet_count, tweet.favorite_count]\n",
    "    col = 0\n",
    "    \n",
    "    for text in data:\n",
    "        sheet.write(row, col, text)\n",
    "        col += 1\n",
    "    row += 1\n",
    "    \n",
    "book.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facebook_scraper import get_posts\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book = xlsxwriter.Workbook('test2.xlsx')\n",
    "sheet = book.add_worksheet('djangogirlsData')\n",
    "row = 0\n",
    "col = 0\n",
    " \n",
    "sheet.write(row, col, \"Post\")\n",
    "row = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in get_posts('djangogirlsbogota', pages=4):\n",
    "    b = TextBlob(post['text'])\n",
    "    \n",
    "    if b.detect_language() == 'es':\n",
    "        print(post['text'])\n",
    "        sheet.write(row, 0, post['text'])\n",
    "        row += 1\n",
    "    \n",
    "book.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_excel = pd.read_excel(open('test1.xlsx','rb'))\n",
    "datos_excel = pd.DataFrame(datos_excel)\n",
    "datos_excel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post = \"\"\n",
    "for data in datos_excel['Tweet']:\n",
    "    Post += data\n",
    "    \n",
    "freq = FreqDist(word_tokenize(Post))\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordList = stopwords.words(\"Spanish\")\n",
    "stopwordList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuationList = list(string.punctuation)\n",
    "punctuationList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwordsList = stopwordList + punctuationList + ['“','”','¡','¿',\"''\",'``','...','→','⬥']\n",
    "stopwordsList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_emoji(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean_emoji = clean_tweets_emoji(Post)\n",
    "tweets_clean_emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets_regex(text):\n",
    "    tweet = re.sub(r'@[a-zA-Z0-9-_.]+', '', text)\n",
    "    tweet = re.sub(r'#[a-zA-Z0-9-_.]+', '', tweet)\n",
    "    tweet = re.sub(r'https://[a-zA-Z0-9-_./]+', '', tweet)\n",
    "    tweet = re.sub(r'www.[a-zA-Z0-9-_./]+', '', tweet)\n",
    "    tweet = re.sub(r'[a-zA-Z0-9-./]+[…]', '', tweet)\n",
    "    tweet = re.sub(r'[ ]+[…]', '', tweet)\n",
    "    tweet = re.sub('\\n|\\r', '', tweet)\n",
    "    tweet = re.sub(r'[á|ä|â|à]', 'a', tweet)\n",
    "    tweet = re.sub(r'[é|ê|è]', 'e', tweet)\n",
    "    tweet = re.sub(r'[í|î|ì]', 'i', tweet)\n",
    "    tweet = re.sub(r'[ó|ô|ò]', 'o', tweet)\n",
    "    tweet = re.sub(r'[ú|û|ù|ü]', 'u', tweet)\n",
    "    tweet = re.sub(r'RT ', '', tweet)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_clean = clean_tweets_regex(tweets_clean_emoji)\n",
    "tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtro = [palabra for palabra in word_tokenize(tweets_clean) if palabra not in stopwordsList]\n",
    "filtro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(filtro)\n",
    "freq.plot(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Process Data\n",
    "\n",
    "### Tweets en Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_excel = pd.read_excel(open('taggedTweets.xlsx','rb'))\n",
    "datos_excel = pd.DataFrame(datos_excel)\n",
    "sample = datos_excel.sample(len(datos_excel))\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construirBolsaDePalabras(palabras):\n",
    "    diccionario={}\n",
    "    for palabra in word_tokenize(palabras):\n",
    "        if palabra not in stopwordsList:\n",
    "            diccionario[palabra]=1\n",
    "    return diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasgosNegativos = []\n",
    "rasgosPositivos = []\n",
    "rasgosNeutrales = []\n",
    "    \n",
    "for indice_fila, fila in sample.iterrows():\n",
    "    tweet_clean = clean_tweets_emoji(fila['Tweet'])\n",
    "    tweet_clean = clean_tweets_regex(tweet_clean)\n",
    "    if fila['Sentimiento'] == 'pos':\n",
    "        rasgosPositivos.append((construirBolsaDePalabras(tweet_clean),fila['Sentimiento']))\n",
    "    elif fila['Sentimiento'] == 'neu':\n",
    "        rasgosNeutrales.append((construirBolsaDePalabras(tweet_clean),fila['Sentimiento']))\n",
    "    else:\n",
    "        rasgosNegativos.append((construirBolsaDePalabras(tweet_clean),fila['Sentimiento']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(rasgosPositivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rasgosPositivos[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "divP=int(len(rasgosPositivos)*0.95)\n",
    "divNeu=int(len(rasgosNeutrales)*0.95)\n",
    "divNeg=int(len(rasgosNegativos)*0.95)\n",
    "clasificadorSentimiento=NaiveBayesClassifier.train(rasgosPositivos[:divNeg]+rasgosNegativos[:divNeg]+rasgosNeutrales[:divNeg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.util.accuracy(clasificadorSentimiento,rasgosPositivos[:divNeg]+rasgosNegativos[:divNeg]+rasgosNeutrales[:divNeg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.util.accuracy(clasificadorSentimiento,rasgosPositivos[divNeg:len(rasgosNegativos)]+rasgosNegativos[divNeg:]+rasgosNeutrales[divNeg:len(rasgosNegativos)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificadorSentimiento.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"¿El peor solo de guitarra de la historia? Burlas a Nick Jonas por su interpretación\"\n",
    "\n",
    "print(tweet)\n",
    "tweet_clean = clean_tweets_emoji(tweet)\n",
    "tweet_clean = clean_tweets_regex(tweet_clean)\n",
    "bolsa=construirBolsaDePalabras(tweet_clean)\n",
    "print(bolsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clasificadorSentimiento.classify(bolsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_excel = pd.read_excel(open('test1.xlsx','rb'))\n",
    "datos_excel = pd.DataFrame(datos_excel)\n",
    "muestra = datos_excel.sample(len(datos_excel))\n",
    "muestra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = []\n",
    "neutral = []\n",
    "negative = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "libro = xlsxwriter.Workbook('test1Tag.xlsx')\n",
    "hoja = libro.add_worksheet('Data')\n",
    "\n",
    "data = [\"Tweet\",\"Sentimiento\"]\n",
    "row = 0\n",
    "col = 0\n",
    " \n",
    "for titulo in data:\n",
    "    hoja.write(row, col, titulo)\n",
    "    col += 1\n",
    "row = 1\n",
    "col = 0\n",
    "\n",
    "for indice_fila, fila in muestra.iterrows():\n",
    "    print(fila[2])\n",
    "    tweet_clean = clean_tweets_emoji(fila[2])\n",
    "    tweet_clean = clean_tweets_regex(tweet_clean)\n",
    "    bolsa=construirBolsaDePalabras(tweet_clean)\n",
    "    sent = clasificadorSentimiento.classify(bolsa)\n",
    "    print(\"\\033[0;31;40m \"+sent+\" \\033[0m\")\n",
    "    hoja.write(indice_fila+row, 0, fila[2])\n",
    "    hoja.write(indice_fila+row, 1, sent)\n",
    "    if sent == 'pos':\n",
    "        positive.append(fila[2])\n",
    "    elif sent == 'neu':\n",
    "        neutral.append(fila[2])\n",
    "    else:\n",
    "        negative.append(fila[2])\n",
    "libro.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excel_data = pd.read_excel(open('test2.xlsx','rb'))\n",
    "excel_data = pd.DataFrame(excel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popularity_list = []\n",
    "num_list = []\n",
    "num = 1\n",
    "\n",
    "for tweet in excel_data['Post']:\n",
    "    print(tweet)\n",
    "    \n",
    "    analysis = TextBlob(tweet)\n",
    "    analysis = analysis.sentiment\n",
    "    print(analysis)\n",
    "    popularity = analysis.polarity\n",
    "    popularity_list.append(popularity)\n",
    "    num_list.append(num)\n",
    "    num = num + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: ShowData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Texto en Español"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(u'Gráfica de barras')\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "nombres = ['Positive','Neutral','Negative']\n",
    "datos = [len(positive),len(neutral),len(negative)]\n",
    "xx = range(len(datos))\n",
    "\n",
    "ax.bar(xx, datos, width=0.8, align='center')\n",
    "ax.set_xticks(xx)\n",
    "ax.set_xticklabels(nombres)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(num_list, popularity_list)\n",
    "#plt.scatter(excel_data['Day'], popularity_list)\n",
    "plt.title(\"Sentiments analysis to PyConCo2020\")\n",
    "plt.xlabel(\"Posts\")\n",
    "plt.ylabel(\"Sentiment\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color=\"white\", colormap=\"Dark2\",\n",
    "               max_font_size=150, random_state=42).generate(tweets_clean)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"WWCode DataPY2019\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
